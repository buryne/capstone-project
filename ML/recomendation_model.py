# -*- coding: utf-8 -*-
"""Recomendation Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vvjh8yigaDPoMvB0VfKa7MKuMOPNfjFu
"""

!git clone https://github.com/tmtyasr/recomendation-systems-model.git

!wget https://raw.githubusercontent.com/tmtyasr/recomendation-systems-model/main/dataset/cleaned_dataset_wisata.csv

#untuk pengolahan data
import pandas as pd
import numpy as np
import requests
from io import StringIO

#URL raw file di Github
Github_url = "https://raw.githubusercontent.com/tmtyasr/recomendation-systems-model/main/dataset/cleaned_dataset_wisata.csv"

#Mendapatkan konten file CSV dari Github
response = requests.get(Github_url)
data = StringIO(response.text)

#Load dataset
wisata = pd.read_csv(data)

#Melihat gambaran data wisata
wisata

wisata.columns

wisata.info()

#menampilkan missing value pada dataset wisata
wisata.isnull().sum()

import os
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split

wisata['City'].head(4)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

#Replace NaN with an empty string
wisata['Place_Name'] = wisata['Place_Name'].fillna('')

# Concatenate 'Place_Name', 'City', 'Category', and 'Rating' into a single text column
wisata['Combined_Info'] = wisata['Place_Name'] + ' ' + wisata['City'] + ' ' + wisata['Category'] + ' ' + wisata['Rating'].astype(str)

# Menggunakan TfIdfVectorizer untuk mengonversi deskripsi menjadi vektor fitur
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(wisata['Combined_Info'])

# Menghitung skor kesamaan kosinus antar tempat wisata
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)

# Construct the required TF-IDF matrix by fitting and transforming the data
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(wisata['Combined_Info'])

#Output the shape of tfidf_matrix
tfidf_matrix.shape

#Construct a reverse map of indices and Place_Name
indices = pd.Series(wisata.index, index=wisata['Place_Name']).drop_duplicates()

# Fungsi untuk mendapatkan rekomendasi berdasarkan tempat wisata tertentu
def get_recommendations(place_name, num_recommendations=10):
    # Dapatkan indeks tempat wisata yang serupa
    place_index = wisata[wisata['Place_Name'] == place_name].index[0]
    similar_places_indices = cosine_similarities[place_index].argsort()[:-num_recommendations-1:-1]

    # Filter recommendations with the same city and category
    input_city = wisata.loc[place_index, 'City']
    input_category = wisata.loc[place_index, 'Category']
    similar_places_indices = [i for i in similar_places_indices if wisata.loc[i, 'City'] == input_city and wisata.loc[i, 'Category'] == input_category]

    # Exclude input Place_Name from the recommendations
    similar_places_indices = [i for i in similar_places_indices if wisata['Place_Name'].iloc[i] != place_name]

    # Return the most similar destinations (Place_Name, City, Category, and Rating)
    return wisata[['Place_Name', 'City', 'Category', 'Rating']].iloc[similar_places_indices]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification

# Custom Callback
class MyCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        # Check the accuracy
        if logs.get('accuracy') is not None and logs.get('accuracy') > 0.98:
            # Stop if threshold is met
            print("\nAccuracy is higher than 0.98, canceling training!")
            self.model.stop_training = True

callbacks = MyCallback()

def create_model(input_dim):
    # Pengurangan kompleksitas model
    model = Sequential()
    model.add(Dense(64, input_dim=input_dim, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    return model

def train_model(model, X_train_scaled, y_train, X_val_scaled, y_val):
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val), callbacks=[callbacks])

    # Evaluasi model di atas data validasi terpisah
    _, val_accuracy = model.evaluate(X_val_scaled, y_val)
    print(f'Validation Accuracy: {val_accuracy}')

    # Print model loss and accuracy
    print("Model Loss:", history.history['loss'][-1])
    print("Model Accuracy:", history.history['accuracy'][-1])

    return model

def save_model(model, save_path='Recommendation_Model.h5'):
    model.save(save_path)
    print(f'Model saved to {save_path}')

# Penambahan data pelatihan
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardisasi fitur untuk meningkatkan konvergensi model
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

# Create model
model = create_model(X_train_scaled.shape[1])

# Train model
trained_model = train_model(model, X_train_scaled, y_train, X_val_scaled, y_val)

# Menampilkan rekomendasi untuk 'Dunia Fantasi' di 'Jakarta'
recommended_places = get_recommendations('Dunia Fantasi')
print(f"Rekomendasi untuk Dunia Fantasi:")
print(recommended_places)

# Save the trained model
save_model(trained_model)

# Download the model
from google.colab import files
files.download("Recommendation_Model.h5")